# rk-llama.cpp

[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)

The [llama.cpp](https://github.com/ggml-org/llama.cpp) fork with the Rockchip NPU integration as a GGML backend.

## Description

This project integrates Rockchip's Neural Processing Unit as a computational backend, enabling energy-efficient, high-performance inference for Large Language Models.

The primary motivation is to leverage the NPU as a powerful accelerator. This approach aims to achieve speedups over CPU-only inference while maintaining comparable accuracy with reduced power consumption.

Currently, the backend is optimized and tested for the **RK3588** SoC. Support for other Rockchip SoCs can be added by updating the configuration file.

## Quick start

1. Clone the repository

```sh
git clone https://github.com/invisiofficial/rk-llama.cpp
cd rk-llama.cpp
```

2. Build the project

```sh
mkdir build && cd build
cmake .. -DLLAMA_RKNPU2=ON
make -j8
```

3. Run inference

```sh
# For Dense models
./build/bin/llama-cli -m ~/Projects/gemma-3-1b-it-Q8_0.gguf

# For MoE models
./build/bin/llama-cli -m ~/Projects/LFM2-8B-A1B-Q4_0.gguf --cpu-moe
```

## Quantizations

### Weights

Weights are converted based on the input type. Implemented types:

`FP16`

Input **F16** weights are directly used in native **FP16** format.

`INT8`

Input **Q8_0** weights are dequantized to FP32, then re-quantized to a uniform per-tensor **INT8** format.

`INT4`

Input **Q4_0** weights are dequantized, rotated using a randomized Hadamard transform (see [2404.00456](https://arxiv.org/abs/2404.00456)), calibrated using a KL-Divergence (see [2411.02530](https://arxiv.org/abs/2411.02530)), and then re-quantized to per-tensor **INT4**.

### Activations

Activations are converted based on the operation type. Implemented types:

`FP16`

Input **F32** activations are converted to **FP16** format.

`INT8`

Input **F32** activations are quantized to **INT8** using per-channel scaling.

`INT4`

Input **F32** activations are rotated using a Hadamard transform and then quantized to **INT4** using per-channel scaling.

### Results

Results (of a matrix multiplication) are converted based on the output type. Implemented types:

`FP32`

**FP32** results from the NPU are already in **F32** and used directly.

`INT32`

**INT32** results from the NPU are dequantized to **F32** using combined weight and activation scales.

`INT16`

**INT16** results from the NPU are dequantized to **F32** with an additional normalization factor for rotated computations.

## Chipsets

### RK3588

The backend supports the following computation types:
*   **W16A16**: FP16 weights & FP16 activations
*   **W8A8**: INT8 weights & INT8 activations
*   **W4A4**: INT4 weights & INT4 activations

## Benchmarks

The following benchmarks were conducted on an RK3588, comparing the performance, accuracy, and power consumption of the NPU backend against the standard CPU (NEON) backend.

| Model | Type | Backend | Perplexity | PP (tok/s) | TG (tok/s) | Power (W) |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Granite4.0 350M** | F16 | CPU | 游릭 20.73췀0.74 | 游댮 154.1췀0.1 | 游릭 25.8췀0.1 | 游댮 6.4췀0.4 |
| | | NPU | 游릭 20.74췀0.74 | 游릭 432.3췀0.6 | 游리 20.2췀0.4 | 游릭 3.2췀0.2 |
| | Q8_0 | CPU | 游릭 20.71췀0.74 | 游댮 163.4췀0.2 | 游릭 40.6췀0.1 | 游댮 6.4췀0.4 |
| | | NPU | 游리 22.68췀0.82 | 游릭 311.8췀2.1 | 游댮 25.4췀0.4 | 游릭 3.6췀0.2 |
| | Q4_0 | CPU | 游릭 24.46췀0.88 | 游릭 340.4췀0.9 | 游릭 55.2췀0.1 | 游댮 6.2췀0.4 |
| | | NPU | 游댮 74.09췀2.87 | 游댮 163.6췀0.2 | 游댮 26.7췀0.5 | 游릭 4.0췀0.2 |
| **Gemma3 1B** | F16 | CPU | 游릭 26.20췀1.08 | 游댮 68.5췀0.1 | 游릭 11.1췀0.1 | 游댮 6.8췀0.4 |
| | | NPU | 游릭 26.18췀1.07 | 游릭 249.6췀0.2 | 游릭 10.8췀0.2 | 游릭 2.8췀0.2 |
| | Q8_0 | CPU | 游릭 26.08췀1.07 | 游댮 73.3췀0.1 | 游릭 19.5췀0.1 | 游댮 7.4췀0.4 |
| | | NPU | 游리 29.15췀1.22 | 游릭 378.6췀0.4 | 游리 16.5췀0.3 | 游릭 3.0췀0.2 |
| | Q4_0 | CPU | 游릭 30.77췀1.31 | 游릭 164.7췀0.2 | 游릭 28.3췀0.1 | 游댮 7.0췀0.4 |
| | | NPU | 游댮 55.53췀2.30 | 游댮 51.4췀0.1 | 游댮 16.7췀0.3 | 游릭 3.0췀0.2 |
| **LFM2 8B A1B** | F16 | CPU | 游릭 15.79췀0.58 | 游리 31.1췀2.9 | 游릭 6.8췀0.2 | 游리 7.0췀0.6 |
| | | NPU | 游릭 15.82췀0.58 | 游릭 38.3췀3.2 | 游릭 6.3췀0.4 | 游릭 5.8췀0.4 |
| | Q8_0 | CPU | 游릭 15.92췀0.59 | 游리 31.7췀0.1 | 游릭 12.9췀0.1 | 游리 7.4췀0.6 |
| | | NPU | 游리 16.76췀0.62 | 游릭 40.7췀0.7 | 游릭 12.5췀0.3 | 游릭 5.8췀0.4 |
| | Q4_0 | CPU | 游릭 18.24췀0.53 | 游릭 62.2췀0.1 | 游릭 22.7췀0.1 | 游리 7.4췀0.6 |
| | | NPU | 游리 26.09췀1.06 | 游리 47.5췀0.1 | 游릭 19.0췀0.1 | 游릭 5.8췀0.4 |

## Contributing

Feel free to open an issue to discuss a bug or feature, or submit a pull request with your improvements.
